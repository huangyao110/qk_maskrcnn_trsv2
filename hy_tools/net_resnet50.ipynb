{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-14T11:48:21.714398Z",
     "start_time": "2023-09-14T11:48:19.590974900Z"
    }
   },
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-14T11:48:21.719399300Z",
     "start_time": "2023-09-14T11:48:21.703391200Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_model_instance_segmentation_resnet50_fpn_v2(num_classes,rpn_s_t=0, box_s_t=0.05):\n",
    "    # load an instance segmentation model pre-trained on COCO\n",
    "    model = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=\"DEFAULT\")\n",
    "\n",
    "    # get number of input features for the classifier\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    # replace the pre-trained head with a new one\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "    # now get the number of input features for the mask classifier\n",
    "    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
    "    hidden_layer = 256\n",
    "    # and replace the mask predictor with a new one\n",
    "    model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask,\n",
    "                                                       hidden_layer,\n",
    "                                                       num_classes)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "====================================================================================================\nLayer (type:depth-idx)                             Output Shape              Param #\n====================================================================================================\nMaskRCNN                                           [100, 4]                  --\n├─GeneralizedRCNNTransform: 1-1                    [2, 3, 800, 800]          --\n├─BackboneWithFPN: 1-2                             [2, 256, 13, 13]          --\n│    └─IntermediateLayerGetter: 2-1                [2, 2048, 25, 25]         --\n│    │    └─Conv2d: 3-1                            [2, 64, 400, 400]         (9,408)\n│    │    └─BatchNorm2d: 3-2                       [2, 64, 400, 400]         (128)\n│    │    └─ReLU: 3-3                              [2, 64, 400, 400]         --\n│    │    └─MaxPool2d: 3-4                         [2, 64, 200, 200]         --\n│    │    └─Sequential: 3-5                        [2, 256, 200, 200]        (215,808)\n│    │    └─Sequential: 3-6                        [2, 512, 100, 100]        1,219,584\n│    │    └─Sequential: 3-7                        [2, 1024, 50, 50]         7,098,368\n│    │    └─Sequential: 3-8                        [2, 2048, 25, 25]         14,964,736\n│    └─FeaturePyramidNetwork: 2-2                  [2, 256, 13, 13]          --\n│    │    └─ModuleList: 3-15                       --                        (recursive)\n│    │    └─ModuleList: 3-16                       --                        (recursive)\n│    │    └─ModuleList: 3-15                       --                        (recursive)\n│    │    └─ModuleList: 3-16                       --                        (recursive)\n│    │    └─ModuleList: 3-15                       --                        (recursive)\n│    │    └─ModuleList: 3-16                       --                        (recursive)\n│    │    └─ModuleList: 3-15                       --                        (recursive)\n│    │    └─ModuleList: 3-16                       --                        (recursive)\n│    │    └─LastLevelMaxPool: 3-17                 [2, 256, 200, 200]        --\n├─RegionProposalNetwork: 1-3                       [1000, 4]                 --\n│    └─RPNHead: 2-3                                [2, 3, 200, 200]          --\n│    │    └─Sequential: 3-18                       [2, 256, 200, 200]        1,180,160\n│    │    └─Conv2d: 3-19                           [2, 3, 200, 200]          771\n│    │    └─Conv2d: 3-20                           [2, 12, 200, 200]         3,084\n│    │    └─Sequential: 3-21                       [2, 256, 100, 100]        (recursive)\n│    │    └─Conv2d: 3-22                           [2, 3, 100, 100]          (recursive)\n│    │    └─Conv2d: 3-23                           [2, 12, 100, 100]         (recursive)\n│    │    └─Sequential: 3-24                       [2, 256, 50, 50]          (recursive)\n│    │    └─Conv2d: 3-25                           [2, 3, 50, 50]            (recursive)\n│    │    └─Conv2d: 3-26                           [2, 12, 50, 50]           (recursive)\n│    │    └─Sequential: 3-27                       [2, 256, 25, 25]          (recursive)\n│    │    └─Conv2d: 3-28                           [2, 3, 25, 25]            (recursive)\n│    │    └─Conv2d: 3-29                           [2, 12, 25, 25]           (recursive)\n│    │    └─Sequential: 3-30                       [2, 256, 13, 13]          (recursive)\n│    │    └─Conv2d: 3-31                           [2, 3, 13, 13]            (recursive)\n│    │    └─Conv2d: 3-32                           [2, 12, 13, 13]           (recursive)\n│    └─AnchorGenerator: 2-4                        [159882, 4]               --\n├─RoIHeads: 1-4                                    [100, 4]                  --\n│    └─MultiScaleRoIAlign: 2-5                     [2000, 256, 7, 7]         --\n│    └─FastRCNNConvFCHead: 2-6                     [2000, 1024]              --\n│    │    └─Conv2dNormActivation: 3-33             [2000, 256, 7, 7]         590,336\n│    │    └─Conv2dNormActivation: 3-34             [2000, 256, 7, 7]         590,336\n│    │    └─Conv2dNormActivation: 3-35             [2000, 256, 7, 7]         590,336\n│    │    └─Conv2dNormActivation: 3-36             [2000, 256, 7, 7]         590,336\n│    │    └─Flatten: 3-37                          [2000, 12544]             --\n│    │    └─Linear: 3-38                           [2000, 1024]              12,846,080\n│    │    └─ReLU: 3-39                             [2000, 1024]              --\n│    └─FastRCNNPredictor: 2-7                      [2000, 2]                 --\n│    │    └─Linear: 3-40                           [2000, 2]                 2,050\n│    │    └─Linear: 3-41                           [2000, 8]                 8,200\n│    └─MultiScaleRoIAlign: 2-8                     [200, 256, 14, 14]        --\n│    └─MaskRCNNHeads: 2-9                          [200, 256, 14, 14]        --\n│    │    └─Conv2dNormActivation: 3-42             [200, 256, 14, 14]        590,336\n│    │    └─Conv2dNormActivation: 3-43             [200, 256, 14, 14]        590,336\n│    │    └─Conv2dNormActivation: 3-44             [200, 256, 14, 14]        590,336\n│    │    └─Conv2dNormActivation: 3-45             [200, 256, 14, 14]        590,336\n│    └─MaskRCNNPredictor: 2-10                     [200, 2, 28, 28]          --\n│    │    └─ConvTranspose2d: 3-46                  [200, 256, 28, 28]        262,400\n│    │    └─ReLU: 3-47                             [200, 256, 28, 28]        --\n│    │    └─Conv2d: 3-48                           [200, 2, 28, 28]          514\n====================================================================================================\nTotal params: 45,880,411\nTrainable params: 45,655,067\nNon-trainable params: 225,344\nTotal mult-adds (Units.GIGABYTES): 693.60\n====================================================================================================\nInput size (MB): 0.01\nForward/backward pass size (MB): 8444.16\nParams size (MB): 183.52\nEstimated Total Size (MB): 8627.69\n===================================================================================================="
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = get_model_instance_segmentation_resnet50_fpn_v2(num_classes=2)\n",
    "from torchinfo import summary\n",
    "summary(model, input_size=(2, 1, 28, 28))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-14T11:49:35.383581900Z",
     "start_time": "2023-09-14T11:49:27.923613700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "def get_model_instance_segmentation_resnet50_fpn(num_classes):\n",
    "    # load an instance segmentation model pre-trained on COCO\n",
    "    model = torchvision.models.detection.maskrcnn_resnet50_fpn(weights = 'DEFAULT')\n",
    "\n",
    "    # get number of input features for the classifier\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    # replace the pre-trained head with a new one\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "    # now get the number of input features for the mask classifier\n",
    "    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
    "    hidden_layer = 256\n",
    "    # and replace the mask predictor with a new one\n",
    "    model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask,\n",
    "                                                       hidden_layer,\n",
    "                                                       num_classes)\n",
    "\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-04T01:12:08.300082900Z",
     "start_time": "2023-09-04T01:12:08.299572700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16477\\anaconda3\\envs\\hy-dl\\Lib\\site-packages\\torch\\nn\\functional.py:3912: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  (torch.floor((input.size(i + 2).float() * torch.tensor(scale_factors[i], dtype=torch.float32)).float()))\n",
      "C:\\Users\\16477\\anaconda3\\envs\\hy-dl\\Lib\\site-packages\\torchvision\\ops\\boxes.py:157: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  boxes_x = torch.min(boxes_x, torch.tensor(width, dtype=boxes.dtype, device=boxes.device))\n",
      "C:\\Users\\16477\\anaconda3\\envs\\hy-dl\\Lib\\site-packages\\torchvision\\ops\\boxes.py:159: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  boxes_y = torch.min(boxes_y, torch.tensor(height, dtype=boxes.dtype, device=boxes.device))\n",
      "C:\\Users\\16477\\anaconda3\\envs\\hy-dl\\Lib\\site-packages\\torch\\__init__.py:1209: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  assert condition, message\n",
      "C:\\Users\\16477\\anaconda3\\envs\\hy-dl\\Lib\\site-packages\\torchvision\\models\\detection\\transform.py:298: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torch.tensor(s, dtype=torch.float32, device=boxes.device)\n",
      "C:\\Users\\16477\\anaconda3\\envs\\hy-dl\\Lib\\site-packages\\torchvision\\models\\detection\\transform.py:299: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  / torch.tensor(s_orig, dtype=torch.float32, device=boxes.device)\n",
      "C:\\Users\\16477\\anaconda3\\envs\\hy-dl\\Lib\\site-packages\\torchvision\\models\\detection\\roi_heads.py:389: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(M + 2 * padding).to(torch.float32) / torch.tensor(M).to(torch.float32)\n",
      "C:\\Users\\16477\\anaconda3\\envs\\hy-dl\\Lib\\site-packages\\torch\\onnx\\symbolic_opset9.py:5589: UserWarning: Exporting aten::index operator of advanced indexing in opset 14 is achieved by combination of multiple ONNX operators, including Reshape, Transpose, Concat, and Gather. If indices include negative values, the exported graph will produce incorrect results.\n",
      "  warnings.warn(\n",
      "C:\\Users\\16477\\anaconda3\\envs\\hy-dl\\Lib\\site-packages\\torch\\onnx\\_internal\\jit_utils.py:306: UserWarning: Constant folding - Only steps=1 can be constant folded for opset >= 10 onnx::Slice op. Constant folding not applied. (Triggered internally at ..\\torch\\csrc\\jit\\passes\\onnx\\constant_fold.cpp:181.)\n",
      "  _C._jit_pass_onnx_node_shape_type_inference(node, params_dict, opset_version)\n",
      "C:\\Users\\16477\\anaconda3\\envs\\hy-dl\\Lib\\site-packages\\torch\\onnx\\utils.py:689: UserWarning: Constant folding - Only steps=1 can be constant folded for opset >= 10 onnx::Slice op. Constant folding not applied. (Triggered internally at ..\\torch\\csrc\\jit\\passes\\onnx\\constant_fold.cpp:181.)\n",
      "  _C._jit_pass_onnx_graph_shape_type_inference(\n",
      "C:\\Users\\16477\\anaconda3\\envs\\hy-dl\\Lib\\site-packages\\torch\\onnx\\utils.py:1186: UserWarning: Constant folding - Only steps=1 can be constant folded for opset >= 10 onnx::Slice op. Constant folding not applied. (Triggered internally at ..\\torch\\csrc\\jit\\passes\\onnx\\constant_fold.cpp:181.)\n",
      "  _C._jit_pass_onnx_graph_shape_type_inference(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============= Diagnostic Run torch.onnx.export version 2.0.1+cu117 =============\n",
      "verbose: False, log level: Level.ERROR\n",
      "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
      "\n",
      "Serving './demo.pth' at http://localhost:8080\n",
      "Serving './demo.pth' at http://localhost:8080\n"
     ]
    },
    {
     "data": {
      "text/plain": "('localhost', 8080)"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 针对有网络模型，但还没有训练保存 .pth 文件的情况\n",
    "import netron\n",
    "import torch.onnx\n",
    "from torch.autograd import Variable\n",
    "from torchvision.models import resnet18  # 以 resnet18 为例\n",
    "\n",
    "myNet = get_model_instance_segmentation_resnet50_fpn_v2(num_classes=3)  # 实例化 resnet18\n",
    "x = torch.randn(2, 3, 40, 40)  # 随机生成一个输入\n",
    "modelData = \"./demo.pth\"  # 定义模型数据保存的路径\n",
    "# modelData = \"./demo.onnx\"  # 有人说应该是 onnx 文件，但我尝试 pth 是可以的 \n",
    "torch.onnx.export(myNet, x, modelData)  # 将 pytorch 模型以 onnx 格式导出并保存\n",
    "netron.start(modelData)  # 输出网络结构\n",
    "\n",
    "#  针对已经存在网络模型 .pth 文件的情况\n",
    "import netron\n",
    "\n",
    "modelData = \"./demo.pth\"  # 定义模型数据保存的路径\n",
    "netron.start(modelData)  # 输出网络结构\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-14T12:03:43.669337Z",
     "start_time": "2023-09-14T12:03:21.692683500Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hy-dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
